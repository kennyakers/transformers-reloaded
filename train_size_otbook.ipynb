{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from torch.nn import Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: mps\n"
     ]
    }
   ],
   "source": [
    "iwslt_dataset = load_dataset('iwslt2017', 'iwslt2017-en-de')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'  # For Apple Silicon GPUs\n",
    "\n",
    "print('DEVICE:', device)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizers\n",
    "tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "tokenizer_de = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "\n",
    "def tokenize(batch):\n",
    "    # Access nested 'translation' and then language specific texts\n",
    "    en_texts = [item['en'] for item in batch['translation']]\n",
    "    de_texts = [item['de'] for item in batch['translation']]\n",
    "    # Apply tokenization and ensure each text is a list of tokens\n",
    "    batch['tokenized_en'] = [list(map(str, tokenizer_en(text))) for text in en_texts]\n",
    "    batch['tokenized_de'] = [list(map(str, tokenizer_de(text))) for text in de_texts]\n",
    "    return batch\n",
    "\n",
    "# Apply the tokenization function\n",
    "iwslt_dataset = iwslt_dataset.map(tokenize, batched=True, batch_size=1000, num_proc=4)\n",
    "\n",
    "# Extract tokenized data\n",
    "en_tokenized_texts = iwslt_dataset['train']['tokenized_en']\n",
    "de_tokenized_texts = iwslt_dataset['train']['tokenized_de']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tokenized_texts, min_freq=1):\n",
    "    # Flatten the list of tokens\n",
    "    all_tokens = list(itertools.chain.from_iterable(tokenized_texts))\n",
    "    \n",
    "    # Count all tokens\n",
    "    token_freqs = Counter(all_tokens)\n",
    "    \n",
    "    # Remove tokens below a certain frequency threshold\n",
    "    vocab = {token: idx + 1 for idx, (token, freq) in enumerate(token_freqs.items()) if freq >= min_freq}\n",
    "    vocab['<pad>'] = 0  # Pad token at index 0\n",
    "    vocab['<unk>'] = len(vocab)  # Unknown token at the last index\n",
    "    return vocab\n",
    "\n",
    "# Build vocabularies\n",
    "EN_VOCAB = build_vocab(en_tokenized_texts)\n",
    "DE_VOCAB = build_vocab(de_tokenized_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    en_batch = [item['tokenized_en'] for item in batch]\n",
    "    de_batch = [['<sos>'] + item['tokenized_de'] + ['<eos>'] for item in batch]\n",
    "    \n",
    "    en_indices = [[EN_VOCAB.get(token, EN_VOCAB['<unk>']) for token in sentence] for sentence in en_batch]\n",
    "    de_indices = [[DE_VOCAB.get(token, DE_VOCAB['<unk>']) for token in sentence] for sentence in de_batch]\n",
    "    \n",
    "    en_tensor = pad_sequence([torch.tensor(seq) for seq in en_indices], padding_value=EN_VOCAB['<pad>'], batch_first=True)\n",
    "    de_tensor = pad_sequence([torch.tensor(seq) for seq in de_indices], padding_value=DE_VOCAB['<pad>'], batch_first=True)\n",
    "    \n",
    "    return {'en': en_tensor.to(device), 'de': de_tensor.to(device)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/torchtext/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding layers\n",
    "d_model = 512\n",
    "\n",
    "en_embedding = torch.nn.Embedding(len(EN_VOCAB), d_model).to(device)\n",
    "de_embedding = torch.nn.Embedding(len(DE_VOCAB), d_model).to(device)\n",
    "\n",
    "\n",
    "model = Transformer(\n",
    "    d_model=d_model,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1,    \n",
    ").to(device)\n",
    "\n",
    "output_projection = torch.nn.Linear(d_model, len(DE_VOCAB)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Dimensions of Feed-Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Feedforward Network:\n",
      "  Layer 1: Input Features = 512, Output Features = 2048\n",
      "  Layer 2: Input Features = 2048, Output Features = 512\n",
      "Decoder Feedforward Network:\n",
      "  Layer 1: Input Features = 512, Output Features = 2048\n",
      "  Layer 2: Input Features = 2048, Output Features = 512\n"
     ]
    }
   ],
   "source": [
    "def print_feedforward_dimensions(transformer):\n",
    "    # Print dimensions for the encoder feed-forward network\n",
    "    if transformer.encoder is not None and len(transformer.encoder.layers) > 0:\n",
    "        encoder_ffn_layer1 = transformer.encoder.layers[0].linear1\n",
    "        encoder_ffn_layer2 = transformer.encoder.layers[0].linear2\n",
    "        print(\"Encoder Feedforward Network:\")\n",
    "        print(f\"  Layer 1: Input Features = {encoder_ffn_layer1.in_features}, Output Features = {encoder_ffn_layer1.out_features}\")\n",
    "        print(f\"  Layer 2: Input Features = {encoder_ffn_layer2.in_features}, Output Features = {encoder_ffn_layer2.out_features}\")\n",
    "\n",
    "    # Print dimensions for the decoder feed-forward network\n",
    "    if transformer.decoder is not None and len(transformer.decoder.layers) > 0:\n",
    "        decoder_ffn_layer1 = transformer.decoder.layers[0].linear1\n",
    "        decoder_ffn_layer2 = transformer.decoder.layers[0].linear2\n",
    "        print(\"Decoder Feedforward Network:\")\n",
    "        print(f\"  Layer 1: Input Features = {decoder_ffn_layer1.in_features}, Output Features = {decoder_ffn_layer1.out_features}\")\n",
    "        print(f\"  Layer 2: Input Features = {decoder_ffn_layer2.in_features}, Output Features = {decoder_ffn_layer2.out_features}\")\n",
    "\n",
    "\n",
    "transformer_model = Transformer()\n",
    "print_feedforward_dimensions(transformer_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Feed-Forward Number of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in feedforward networks: 25196544\n"
     ]
    }
   ],
   "source": [
    "def count_ffn_parameters(transformer):\n",
    "    total_params = 0\n",
    "    # Assume each encoder and decoder layer has identical feedforward networks, so we just need to calculate for one and multiply\n",
    "    if transformer.encoder is not None and len(transformer.encoder.layers) > 0:\n",
    "        # Calculate for one layer in the encoder\n",
    "        encoder_ffn_layer1 = transformer.encoder.layers[0].linear1\n",
    "        encoder_ffn_layer2 = transformer.encoder.layers[0].linear2\n",
    "        # Layer 1 parameters\n",
    "        params_layer1 = encoder_ffn_layer1.weight.numel() + encoder_ffn_layer1.bias.numel()\n",
    "        # Layer 2 parameters\n",
    "        params_layer2 = encoder_ffn_layer2.weight.numel() + encoder_ffn_layer2.bias.numel()\n",
    "        # Total parameters for one encoder layer's feedforward network\n",
    "        total_params_per_layer = params_layer1 + params_layer2\n",
    "    \n",
    "    # Multiply by the total number of encoder and decoder layers\n",
    "    num_layers = len(transformer.encoder.layers) + len(transformer.decoder.layers)\n",
    "    # Total parameters in all feedforward networks\n",
    "    total_params = total_params_per_layer * num_layers\n",
    "\n",
    "    return total_params\n",
    "\n",
    "total_ffn_params = count_ffn_parameters(model)\n",
    "print(\"Total parameters in feedforward networks:\", total_ffn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting All Parameters in the Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 44140544\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total parameters:\", count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(iwslt_dataset['train'], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(iwslt_dataset['validation'], batch_size=BATCH_SIZE,collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(output_projection.parameters()), lr=1e-4)\n",
    "\n",
    "# This is closer to the papers optimizer\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     list(model.parameters()) + list(output_projection.parameters()),\n",
    "#     lr=1e-4,  # This initial learning rate is a placeholder, it will be updated by the scheduler\n",
    "#     betas=(0.9, 0.98),  # β1 and β2\n",
    "#     eps=1e-9  # ϵ\n",
    "# )\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        src_tensor = batch['en']\n",
    "        tgt_tensor = batch['de']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        src = en_embedding(src_tensor)\n",
    "        tgt = de_embedding(tgt_tensor)\n",
    "\n",
    "        # Adjust the sequence length compatibility as before\n",
    "        if src.shape[1] > tgt.shape[1]:\n",
    "            src = src[:, :tgt.shape[1], :]\n",
    "        elif src.shape[1] < tgt.shape[1]:\n",
    "            pad_size = tgt.shape[1] - src.shape[1]\n",
    "            src = torch.nn.functional.pad(src, (0, 0, 0, pad_size), value=EN_VOCAB['<pad>'])\n",
    "\n",
    "        out = model(src, tgt)\n",
    "        out = output_projection(out)  # Project output to the DE vocabulary size\n",
    "\n",
    "        # New code for calculating loss, ignoring padding\n",
    "        target_mask = (tgt_tensor != DE_VOCAB['<pad>']).view(-1)\n",
    "        loss = torch.nn.functional.cross_entropy(out.view(-1, len(DE_VOCAB)), tgt_tensor.view(-1), reduction='none')\n",
    "        loss = (loss * target_mask).sum() / target_mask.sum()\n",
    "\n",
    "        # if torch.backends.mps.is_available():\n",
    "        #     torch.mps.empty_cache()\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "    # Save a checkpoint at the end of each epoch\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'output_projection_state_dict': output_projection.state_dict(),  # Correct key for output projection state\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'vocab_en': EN_VOCAB,\n",
    "        'vocab_de': DE_VOCAB,\n",
    "        'settings': {\n",
    "            'd_model': d_model,\n",
    "            'nhead': 8,\n",
    "            'num_encoder_layers': 6,\n",
    "            'num_decoder_layers': 6,\n",
    "            'dim_feedforward': 2048,\n",
    "            'output_vocab_size': len(DE_VOCAB)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, f'model_ckpt/transformer_checkpoint_epoch{epoch}.pth')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
